# Job Deployment Feature

## Overview

The Job Deployment feature enables deploying Kindling data apps as Spark jobs across Fabric, Databricks, and Synapse platforms. This is a **core framework capability** available to all users, not just for testing infrastructure.

## Architecture

### Design Principles

1. **Platform Services Pattern** - No if/else platform differentiation in framework code
2. **Dependency Injection** - Platform services injected, not instantiated
3. **Single Responsibility** - Framework orchestrates, platforms execute
4. **Extensibility** - Add new platforms by implementing interface
5. **Testability** - Mock platform services for unit tests

### Components

#### 1. PlatformService Interface Extensions

Added to `PlatformService` abstract class:

```python
@abstractmethod
def deploy_spark_job(app_files: Dict[str, str], job_config: Dict[str, Any]) -> Dict[str, Any]:
    """Deploy application as Spark job - returns {job_id, deployment_path, metadata}"""

@abstractmethod
def run_spark_job(job_id: str, parameters: Dict[str, Any] = None) -> str:
    """Execute deployed job - returns run_id for monitoring"""

@abstractmethod
def get_job_status(run_id: str) -> Dict[str, Any]:
    """Get job status - returns {status, start_time, end_time, error, logs}"""

@abstractmethod
def cancel_job(run_id: str) -> bool:
    """Cancel running job - returns True if successful"""
```

#### 2. DataAppDeployer Orchestrator

Location: `packages/kindling/job_deployment.py`

Platform-agnostic orchestrator that:
- Validates job configuration
- Prepares app files from directory or .kda package
- Injects bootstrap shim automatically
- Delegates platform-specific operations to injected platform service
- **Zero platform conditionals** - all platform logic in services

#### 3. Platform Implementations

**Fabric** (`packages/kindling/platform_fabric.py`):
- Creates Spark Job Definition via REST API
- Uploads files to OneLake using Create → Append → Flush pattern
- Updates job definition with abfss:// file paths
- Executes jobs via Item Run API
- Monitors job status via Operations API

**Databricks** (`packages/kindling/platform_databricks.py`):
- Uploads files to DBFS via dbutils
- Creates Databricks job via Jobs API 2.1
- Supports existing cluster or new cluster config
- Executes via run-now API
- Monitors via runs/get API

**Synapse** (`packages/kindling/platform_synapse.py`):
- Uploads files to workspace storage via mssparkutils
- Creates Spark Job Definition via Artifacts API
- Submits as Spark batch job
- Monitors via batch job API
- Supports Spark pool configuration

#### 4. Bootstrap Shim Pattern

Generated by DataAppDeployer, platform-agnostic:
- Detects storage utils (mssparkutils or dbutils)
- Installs framework wheels from artifacts storage
- Executes main application entry point
- Clean execution flow: shim → framework → app

## Usage

### In Notebook

```python
from kindling.job_deployment import DataAppDeployer

# DI automatically injects platform service
deployer = DataAppDeployer()

# Deploy app as job
result = deployer.deploy_as_job(
    app_path="/path/to/app",  # Directory or .kda file
    job_config={
        "job_name": "my-data-app",
        "lakehouse_id": "...",  # Fabric
        "artifacts_path": "abfss://.../artifacts"
    }
)

# Run the job
run_id = deployer.run_job(result['job_id'])

# Monitor status
status = deployer.get_job_status(run_id)
print(f"Job status: {status['status']}")

# Cancel if needed
if status['status'] == 'Running':
    deployer.cancel_job(run_id)
```

### In Local Script or CLI

```python
from kindling.job_deployment import DataAppDeployer
from kindling.platform_fabric import FabricService
from kindling.platform_provider import SparkPlatformServiceProvider
from kindling.spark_log import SparkLogger

# Manual setup when not in notebook
logger = SparkLogger(name="deployer")
platform_service = FabricService(config, logger)

provider = SparkPlatformServiceProvider()
provider.set_service(platform_service)

logger_provider = ...  # Create logger provider

# Create deployer with manual DI
deployer = DataAppDeployer(provider, logger_provider)

# Use same API
result = deployer.deploy_as_job(app_path, job_config)
```

## Job Configuration

### Fabric

```python
job_config = {
    "job_name": "my-app",  # Required
    "lakehouse_id": "...",  # Required - Lakehouse for storage
    "artifacts_path": "abfss://.../artifacts",  # Framework wheels location
    "entry_point": "bootstrap_shim.py",  # Main file (default)
    "executor_cores": 4,
    "executor_memory": "28g",
    "driver_cores": 4,
    "driver_memory": "28g"
}
```

### Databricks

```python
job_config = {
    "job_name": "my-app",  # Required
    "cluster_id": "...",  # Optional - existing cluster
    "new_cluster": {  # Optional - new cluster config
        "spark_version": "13.3.x-scala2.12",
        "node_type_id": "Standard_DS3_v2",
        "num_workers": 2
    },
    "entry_point": "bootstrap_shim.py",  # Main file (default)
    "libraries": []  # Additional libraries
}
```

### Synapse

```python
job_config = {
    "job_name": "my-app",  # Required
    "spark_pool_name": "...",  # Required - Spark pool name
    "entry_point": "bootstrap_shim.py",  # Main file (default)
    "executors": 2,
    "executor_cores": 4,
    "executor_memory": "28g",
    "driver_cores": 4,
    "driver_memory": "28g"
}
```

## Testing

Unit tests: `tests/unit/test_job_deployment.py`
- 8 tests, all passing
- 86% code coverage for job_deployment.py
- Tests validate config, file preparation, bootstrap shim generation, delegation to platform services

System tests: `tests/system/runners/fabric_runner.py`
- Example FabricTestRunner using framework deployment
- Demonstrates production usage pattern
- Validates that deployment works end-to-end

## Implementation Files

1. `packages/kindling/notebook_framework.py` - PlatformService interface extensions
2. `packages/kindling/job_deployment.py` - DataAppDeployer orchestrator
3. `packages/kindling/platform_fabric.py` - Fabric implementation
4. `packages/kindling/platform_databricks.py` - Databricks implementation
5. `packages/kindling/platform_synapse.py` - Synapse implementation
6. `tests/unit/test_job_deployment.py` - Unit tests
7. `tests/system/runners/fabric_runner.py` - System test runner example

## Benefits

1. **Production-Ready** - Same code for testing and production deployment
2. **Platform-Agnostic** - Write once, deploy anywhere
3. **Extensible** - Add new platforms without changing framework code
4. **Testable** - Mock platform services for isolated testing
5. **Consistent** - Same API across all platforms
6. **Observable** - Monitor job status, logs, and errors
7. **Maintainable** - Clean separation of concerns

## Future Enhancements

- [ ] Add job scheduling capabilities
- [ ] Support for job dependencies (DAGs)
- [ ] Job templates for common patterns
- [ ] Integration with CI/CD pipelines
- [ ] Job performance monitoring and optimization
- [ ] Automatic retry logic
- [ ] Job result collection and storage
